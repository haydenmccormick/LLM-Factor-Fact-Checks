import argparse
from rouge_score import rouge_scorer
import json
from tqdm import tqdm

def format_prediction(prediction):
    """
    Parse prediction generated by LLM
    """
    prediction = prediction.split('\n')
    try:
        claim = [p for p in prediction if "CLAIM:" in p][0] \
            .split("CLAIM: ")[1] \
            .split("CLAIMANT:")[0]
    except:
        claim = ""
    try:
        claimant = [p for p in prediction if "CLAIMANT:" in p][0] \
            .split("CLAIMANT: ")[1] \
            .split("VERDICT:")[0]
    except:
        claimant = ""
    try:
        verdict = [p for p in prediction if "VERDICT:" in p][0] \
            .split("VERDICT: ")[1] \
            .split('<|eot_id|>')[0] \
            .split('\n')[0]
    except:
        verdict = ""
    return claim, claimant, verdict

def evaluate(dataset):
    """
    Print evaluation results across each class.

    Args:
    --dataset: Path to the .jsonl dataset file
    """
    with open(dataset, "r") as f:
        lines = f.readlines()

    scorer = rouge_scorer.RougeScorer(['rouge1'])
    claim_precisions, claim_recalls, claim_f1s = [], [], []
    claimant_precisions, claimant_recalls, claimant_f1s = [], [], []
    verdict_precisions, verdict_recalls, verdict_f1s = [], [], []

    for line in tqdm(lines[:587]):
        data = json.loads(line)
        prediction = data["prediction"]
        claim, claimant, verdict = format_prediction(prediction)
        gold_claim = data["claim"]
        gold_claimant = data["claimant"]
        gold_verdict = data["verdict"]

        claim_score = scorer.score(gold_claim, claim)
        claimant_score = scorer.score(gold_claimant, claimant)
        verdict_score = scorer.score(gold_verdict, verdict)

        claim_precisions.append(claim_score["rouge1"].precision)
        claim_recalls.append(claim_score["rouge1"].recall)
        claim_f1s.append(claim_score["rouge1"].fmeasure)

        claimant_precisions.append(claimant_score["rouge1"].precision)
        claimant_recalls.append(claimant_score["rouge1"].recall)
        claimant_f1s.append(claimant_score["rouge1"].fmeasure)

        verdict_precisions.append(verdict_score["rouge1"].precision)
        verdict_recalls.append(verdict_score["rouge1"].recall)
        verdict_f1s.append(verdict_score["rouge1"].fmeasure)

    print("Claim F1:", sum(claim_f1s)/len(claim_f1s))
    print("Claim Precision:", sum(claim_precisions)/len(claim_precisions))
    print("Claim Recall:", sum(claim_recalls)/len(claim_recalls))
    print("-----------------")
    print("Claimant F1:", sum(claimant_f1s)/len(claimant_f1s))
    print("Claimant Precision:", sum(claimant_precisions)/len(claimant_precisions))
    print("Claimant Recall:", sum(claimant_recalls)/len(claimant_recalls))
    print("-----------------")
    print("Verdict F1:", sum(verdict_f1s)/len(verdict_f1s))
    print("Verdict Precision:", sum(verdict_precisions)/len(verdict_precisions))
    print("Verdict Recall:", sum(verdict_recalls)/len(verdict_recalls))

if __name__ == "__main__":
    """
    Required arguments:
    --dataset: Path to the dataset file
    """
    parser = argparse.ArgumentParser(description="Evaluate predictions on a dataset.")
    parser.add_argument("--dataset", type=str, required=True, help="Path to the dataset file")
    args = parser.parse_args()

    evaluate(args.dataset)
